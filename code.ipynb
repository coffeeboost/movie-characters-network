{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Sc0m8G3ZwM9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo apt-get install graphviz libgraphviz-dev\n",
        "# !pip install transformers tqdm chart_studio pygraphviz --upgrade --quiet"
      ],
      "metadata": {
        "id": "a7jFTdt4z08w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS TO RUN NLP CLASSIFIERS\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# # IMPORTS\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from itertools import permutations, product\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import plotly.express as px\n",
        "import chart_studio.plotly as py\n",
        "\n",
        "# # CONNECT TO YOUR GOOGLE DRIVE TO SAVE DATA\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# # GLOBAL VARIABLES\n",
        "movie_scripts_path = \"/content/drive/MyDrive/honours/movie scripts/\"\n",
        "movie_results_path = \"/content/drive/MyDrive/honours/results/\"\n",
        "movie_norm_results_path = \"/content/drive/MyDrive/honours/norm_results/\"\n",
        "EMOTION_COLUMNS =  ['Sentiment','Intimacy','Irony','Hate','admiration',\n",
        " 'amusement',\n",
        " 'anger',\n",
        " 'annoyance',\n",
        " 'approval',\n",
        " 'caring',\n",
        " 'confusion',\n",
        " 'curiosity',\n",
        " 'desire',\n",
        " 'disappointment',\n",
        " 'disapproval',\n",
        " 'disgust',\n",
        " 'embarrassment',\n",
        " 'excitement',\n",
        " 'fear',\n",
        " 'gratitude',\n",
        " 'grief',\n",
        " 'joy',\n",
        " 'love',\n",
        " 'nervousness',\n",
        " 'optimism',\n",
        " 'pride',\n",
        " 'realization',\n",
        " 'relief',\n",
        " 'remorse',\n",
        " 'sadness',\n",
        " 'surprise',\n",
        " 'neutral']"
      ],
      "metadata": {
        "id": "K3hf8GzVwIrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331f60e9-6e87-4211-9a29-26531285a483"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load models for different emotions [once!]\n",
        "\n",
        "# add any models here\n",
        "MODELS = ['cardiffnlp/twitter-roberta-base-hate',\n",
        "          'cardiffnlp/twitter-roberta-base-irony',\n",
        "          'cardiffnlp/twitter-roberta-base-intimacy-latest',\n",
        "          'SamLowe/roberta-base-go_emotions']\n",
        "\n",
        "for MODEL in MODELS:\n",
        "  # load model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL, truncation=True, max_length=512)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "  # save model (once)\n",
        "  tokenizer.save_pretrained(MODEL)\n",
        "  model.save_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "LbLWq_SGSdNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util functions"
      ],
      "metadata": {
        "id": "H5yxR-4Ak3-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bHlKODMsn2vY"
      },
      "outputs": [],
      "source": [
        "def invalid_name(word_before_colon):\n",
        "  if len(word_before_colon) == 0 or word_before_colon[0] == '[' or word_before_colon[0] == '(':\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def extract_names(script, threshold=10):\n",
        "  lines_per_character = {}\n",
        "\n",
        "  # loop through every line\n",
        "  for line in script:\n",
        "    parts = line.split(':')\n",
        "    word_before_colon = parts[0].strip()\n",
        "    if invalid_name(word_before_colon): continue\n",
        "\n",
        "    # count number of lines for every character\n",
        "    if word_before_colon in lines_per_character:\n",
        "      lines_per_character[word_before_colon] = lines_per_character[word_before_colon] + 1\n",
        "    else:\n",
        "      lines_per_character[word_before_colon] = 0\n",
        "\n",
        "  # filter for characters with a certain number of lines\n",
        "  filtered_dict = {key: value for key, value in lines_per_character.items() if value > threshold}\n",
        "  character_list = list(filtered_dict.keys())\n",
        "\n",
        "  return character_list, lines_per_character\n",
        "\n",
        "def score_emotion(tokenizer, model, text):\n",
        "  encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "  output = model(**encoded_input)\n",
        "  scores = output[0][0].detach().numpy()\n",
        "\n",
        "  return scores\n",
        "\n",
        "def irony_sentiment(text='Hello World'):\n",
        "  path = '/content/cardiffnlp/twitter-roberta-base-irony'\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(path)\n",
        "\n",
        "  return score_emotion(tokenizer, model, text)\n",
        "\n",
        "def hate_sentiment(text=\"Good night ðŸ˜Š\"):\n",
        "  path = '/content/cardiffnlp/twitter-roberta-base-hate'\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(path)\n",
        "\n",
        "  return score_emotion(tokenizer, model, text)\n",
        "\n",
        "def intimacy_sentiment(text=\"The cookie taste delicious\"):\n",
        "  path = \"cardiffnlp/twitter-roberta-base-intimacy-latest\"\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(path)\n",
        "\n",
        "  return model(**tokenizer(text, return_tensors=\"pt\")).logits.item()\n",
        "\n",
        "def emotion_25_sentiment(text=\"I am not having a good day\"):\n",
        "  path = \"SamLowe/roberta-base-go_emotions\"\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(path)\n",
        "\n",
        "  return model(**tokenizer(text, return_tensors=\"pt\")).logits\n",
        "\n",
        "def sentiment_magic(choice, line, debug=False):\n",
        "    if choice == 'sentiment':\n",
        "      polarity = TextBlob(line).sentiment.polarity\n",
        "      if debug:\n",
        "        threshold = 0.1\n",
        "        percentage = 1.0\n",
        "      return polarity\n",
        "\n",
        "    if choice == 'hate':\n",
        "      return hate_sentiment(line)\n",
        "\n",
        "    if choice == 'irony':\n",
        "      return irony_sentiment(line)\n",
        "\n",
        "    if choice == 'intimacy':\n",
        "      return intimacy_sentiment(line)\n",
        "\n",
        "    if choice == '25 emotions':\n",
        "      return pd.Series(emotion_25_sentiment(line)[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script data extract and load to Google drive"
      ],
      "metadata": {
        "id": "dueM8C015EC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_extracter(path, method):\n",
        "  # Selects top k characters. Also removes obiously wrong names.\n",
        "  THRESHOLD = 10\n",
        "\n",
        "  if method == 'Harry-Potter-Parsing':\n",
        "    # Open Harry Potter data from Kaggle\n",
        "    df = pd.read_csv(path)\n",
        "    df = df[['character', 'dialog']]\n",
        "\n",
        "    # Rename columns to match my conventions\n",
        "    df = df.rename(columns={'character': 'Name', 'dialog': 'Line'})\n",
        "\n",
        "    # Filter the top k characters\n",
        "    grouped_counts = df.groupby('Name').size().reset_index(name='Count')\n",
        "    top_k_counts = grouped_counts.sort_values(by='Count', ascending=False).head(THRESHOLD)\n",
        "    df = df[df['Name'].isin(top_k_counts['Name'])]\n",
        "\n",
        "  if method == 'Colon-based-parsing':\n",
        "    # Open txt file from path\n",
        "    with open(path, 'r') as file:\n",
        "        script = [line.strip() for line in file.readlines()]\n",
        "\n",
        "    # Assume all names are before the colon\n",
        "    names, lines_per_name = extract_names(script, threshold=THRESHOLD)\n",
        "\n",
        "    # Assume all lines are after the colon\n",
        "    df = pd.DataFrame({'Name': [line.split(':')[0].strip() for line in script],\n",
        "                      'Line': [line.strip().split(':')[1].strip() for line in script]})\n",
        "\n",
        "    # Filter for the top k names. Removes obviously wrong names.\n",
        "    df = df[df['Name'].isin(names)]\n",
        "\n",
        "  if method == 'IMSDB-parser':\n",
        "    # Fetch data\n",
        "    response_all_scripts = requests.get(path)\n",
        "    soup = BeautifulSoup(response_all_scripts.text, 'html.parser')\n",
        "    child_elements = soup.find_all('pre')\n",
        "    x = child_elements[-1].contents # get the content of the innermost 'pre'\n",
        "\n",
        "    df = pd.DataFrame(columns=['Name', 'Line'])\n",
        "    data = []\n",
        "\n",
        "    # Iterate through child elements and add them to the list\n",
        "    for index, element in enumerate(x):\n",
        "      if element.name == 'b':\n",
        "        name = element.get_text(strip=True)\n",
        "        try:\n",
        "          if x[index+1] != None:\n",
        "            line = x[index+1].strip()\n",
        "        except Exception as e:\n",
        "          line = \"\"\n",
        "        data.append({'Name': name, 'Line': line})\n",
        "\n",
        "    # Save result as a dataframe\n",
        "    df = pd.concat([pd.DataFrame([entry]) for entry in data], ignore_index=True)\n",
        "\n",
        "    # Apply a small threshold to remove very obviously wrong names\n",
        "    df = df[df.groupby('Name')['Name'].transform('count') > THRESHOLD]\n",
        "    df = df[(df['Name'] != '') & (df['Line']!='')]\n",
        "\n",
        "  # Final check to remove nulls\n",
        "  df = df.dropna()\n",
        "  return df\n",
        "\n",
        "def delete_files_in_dir(path):\n",
        "  file_list = os.listdir(path)\n",
        "\n",
        "  for file_name in file_list:\n",
        "      file_path = os.path.join(movie_results_path, file_name)\n",
        "      try:\n",
        "          os.remove(file_path)\n",
        "          print(f\"Deleted: {file_path}\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "def update_dataframe(path, col_name, emotion):\n",
        "  df = pd.read_csv(path)\n",
        "  df = df[df['Line'].notnull()]\n",
        "  df[col_name] = df['Line'].progress_apply(lambda x: sentiment_magic(emotion, x))\n",
        "  return df\n",
        "\n",
        "## EXAMPLE - extract script to dataframe\n",
        "# path = movie_scripts_path + 'civil_war.txt'\n",
        "# df = data_extracter(path, method='Colon-based-parsing')\n",
        "# df.to_csv(movie_results_path + 'civil_war.csv', index=False)\n",
        "\n",
        "## EXAMPLE - extract script from IMSDB website\n",
        "# path = 'https://imsdb.com/scripts/Joker.html'\n",
        "# data_extracter(path, 'IMSDB-parser')\n",
        "\n",
        "## EXAMPLE - clear files in the movie results directory\n",
        "# delete_files_in_dir(movie_results_path)\n",
        "\n",
        "## EXAMPLE - update dataframes with new sentiment column\n",
        "# path = movie_results_path + 'civil_war.csv'\n",
        "# df = update_dataframe(path, 'Intimacy', 'intimacy')\n",
        "# df.to_csv(path, index=False) #uncomment to overwrite the saved file\n",
        "\n",
        "## EXAMPLE - update dataframe with 25 emotions\n",
        "# path = movie_results_path+'civil_war.csv'\n",
        "# df = pd.read_csv(path)\n",
        "# df = update_dataframe(path, ['admiration',...,'neutral'], '25 emotions')\n",
        "# df.to_csv(path, index=False) #uncomment to overwrite the saved file"
      ],
      "metadata": {
        "id": "UTuC1LGZ3kcE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "- filter top k characters by number of lines\n",
        "- filter length of line to greater than a threhold\n",
        "- concat/average concurrent same speaker\n",
        "- calculate average sentiment for every pair of consecutive speakers\n",
        "- visualize sentiment over time for all characters + host online\n",
        "- average weights of incoming/outoing nodes per node\n",
        "- my_min_cut_decomposition + visualization\n",
        "- community algorithms + visualization"
      ],
      "metadata": {
        "id": "m_ssyvIuVtxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter top k characters by number of lines\n",
        "def filter_top_k_characters(df, threshold=16):\n",
        "  # Get the top k names\n",
        "  name_counts = df['Name'].value_counts()\n",
        "  top_k_names = name_counts.head(threshold).index.tolist()\n",
        "  filtered_df = df[df['Name'].isin(top_k_names)]\n",
        "  return filtered_df\n",
        "\n",
        "# filter length of line to greater than threshold\n",
        "def filter_length_line(df, threshold=2):\n",
        "  filtered_df = df[df['Line'].apply(lambda x: len(x)) > threshold]\n",
        "  return filtered_df\n",
        "\n",
        "def calculate_average_sentiment(pair_sentiment, pair_count):\n",
        "  # Calculate the average sentiment for each pair of characters\n",
        "  average_sentiment = {}\n",
        "  for pair in pair_sentiment:\n",
        "      if pair_count[pair] > 0:\n",
        "          average_sentiment[pair] = pair_sentiment[pair] / pair_count[pair]\n",
        "      else:\n",
        "          average_sentiment[pair] = 0\n",
        "\n",
        "  # Create a new DataFrame from pairs and average sentiment\n",
        "  result_data = {\n",
        "      'Character Pair': list(average_sentiment.keys()),\n",
        "      'Average Sentiment': list(average_sentiment.values())\n",
        "  }\n",
        "\n",
        "  result_df = pd.DataFrame(result_data)\n",
        "  return result_df\n",
        "\n",
        "# Calculate the average sentiment for each pair of consecutive characters\n",
        "def calculate_average_sentiment_per_consecutive_pair(df, annotated, emotion='Sentiment'):\n",
        "  pair_sentiment = {}\n",
        "  pair_count = {}\n",
        "\n",
        "  # Loop through and count sentiments\n",
        "  for i in range(len(df)):\n",
        "      current_char = df.iloc[i, df.columns.get_loc('Name')]\n",
        "      next_char = df.iloc[i, df.columns.get_loc('Spoken')] if annotated else df.iloc[i + 1, df.columns.get_loc('Name')]\n",
        "      sentiment = df.iloc[i, df.columns.get_loc(emotion)]\n",
        "      pair = (current_char, next_char)\n",
        "\n",
        "      if pair not in pair_sentiment:\n",
        "        pair_sentiment[pair] = sentiment\n",
        "        pair_count[pair] = 1\n",
        "      else:\n",
        "        pair_sentiment[pair] += sentiment\n",
        "        pair_count[pair] += 1\n",
        "\n",
        "  result_df = calculate_average_sentiment(pair_sentiment, pair_count)\n",
        "  return result_df\n",
        "\n",
        "\n",
        "def create_graph_average_sentiment_per_consecutive_pair(df):\n",
        "  G = nx.DiGraph()\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "      char_pair = row['Character Pair']\n",
        "      avg_sentiment = row['Average Sentiment']\n",
        "\n",
        "      if avg_sentiment >= 0:\n",
        "        edge_color = 'green'\n",
        "      else:\n",
        "        edge_color = 'red'\n",
        "\n",
        "      # width is scaled to magnitude\n",
        "      edge_width = abs(avg_sentiment)\n",
        "\n",
        "      # add character and weight to graph\n",
        "      G.add_edge(char_pair[0], char_pair[1], weight=avg_sentiment, capacity=avg_sentiment, color=edge_color, width=edge_width)\n",
        "\n",
        "  return G\n",
        "\n",
        "def plot_average_sentiment_per_consecutive_pair(G, weight_scale=3):\n",
        "  pos = nx.circular_layout(G)\n",
        "  edge_colors = [G[u][v]['color'] for u, v in G.edges()]\n",
        "  edge_widths = [G[u][v]['width']*weight_scale for u, v in G.edges()]\n",
        "  nx.draw(G, pos, with_labels=True, edge_color=edge_colors, width=edge_widths)\n",
        "  plt.show()\n",
        "\n",
        "def my_min_cut(G):\n",
        "  min_cut_value = float('inf')\n",
        "  min_cut_partition = ()\n",
        "\n",
        "  for source in list(G.nodes()):\n",
        "    for sink in list(G.nodes()):\n",
        "      if source != sink:\n",
        "        cut_value, partition = nx.minimum_cut(G, source, sink)\n",
        "        if cut_value < min_cut_value:\n",
        "          min_cut_value = cut_value\n",
        "          min_cut_partition = partition\n",
        "  return min_cut_value, min_cut_partition\n",
        "\n",
        "\n",
        "def my_min_cut_decomposition(G, num_iterations=7, weight_scale=3):\n",
        "  for step in range(num_iterations): # number of iterations\n",
        "    # Create subgraphs based on the partition obtained\n",
        "    min_cut_value, partition = my_min_cut(G)\n",
        "    subgraph1_nodes = partition[0]\n",
        "    subgraph2_nodes = partition[1]\n",
        "\n",
        "    subgraph1 = G.subgraph(subgraph1_nodes)\n",
        "    subgraph2 = G.subgraph(subgraph2_nodes)\n",
        "\n",
        "    # Display subgraphs\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    pos = nx.circular_layout(subgraph1)\n",
        "    # Use force_atlas2_layout for better positioning without overlaps\n",
        "    pos = graphviz_layout(subgraph1, prog=\"neato\", root=None)\n",
        "\n",
        "    nx.draw(subgraph1, pos, with_labels=True, edge_color=['red' if d['weight'] < 0 else 'green' for u, v, d in subgraph1.edges(data=True)],\n",
        "            width=[abs(d['weight']) * weight_scale for u, v, d in subgraph1.edges(data=True)], node_color='lightgreen', node_size=500, font_weight='bold')\n",
        "    plt.title(f\"Subgraph 1 (Step {step+1})\")\n",
        "\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    pos = nx.circular_layout(subgraph2)\n",
        "    # Use force_atlas2_layout for better positioning without overlaps\n",
        "    pos = graphviz_layout(subgraph2, prog=\"neato\", root=None)\n",
        "    nx.draw(subgraph2, pos, with_labels=True, edge_color=['red' if d['weight'] < 0 else 'green' for u, v, d in subgraph2.edges(data=True)],\n",
        "            width=[abs(d['weight']) * weight_scale for u, v, d in subgraph2.edges(data=True)], node_color='lightgreen', node_size=500, font_weight='bold')\n",
        "    plt.title(f\"Subgraph 2 - Step {step+1}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Replace G with the larger subgraph\n",
        "    if len(subgraph1) >= len(subgraph2):\n",
        "      G = subgraph1.copy()\n",
        "    else:\n",
        "      G = subgraph2.copy()\n",
        "\n",
        "def average_incoming_outgoing_weights_of_every_node(G):\n",
        "  # Calculate the average weights for incoming and outgoing edges for each node\n",
        "  average_incoming_weights = {}\n",
        "  average_outgoing_weights = {}\n",
        "\n",
        "  for node in G.nodes():\n",
        "      incoming_weights = [G[predecessor][node]['weight'] for predecessor in G.predecessors(node)]\n",
        "      outgoing_weights = [G[node][successor]['weight'] for successor in G.successors(node)]\n",
        "\n",
        "      average_incoming_weights[node] = sum(incoming_weights) / len(incoming_weights) if incoming_weights else 0\n",
        "      average_outgoing_weights[node] = sum(outgoing_weights) / len(outgoing_weights) if outgoing_weights else 0\n",
        "\n",
        "  # Create a DataFrame\n",
        "  avg_df = pd.DataFrame({\n",
        "      'Character': list(G.nodes()),\n",
        "      'Average_Incoming_Weight': [average_incoming_weights[node] for node in G.nodes()],\n",
        "      'Average_Outgoing_Weight': [average_outgoing_weights[node] for node in G.nodes()]\n",
        "  })\n",
        "\n",
        "  return avg_df\n"
      ],
      "metadata": {
        "id": "I-VsoO1n9WaW"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # EXAMPLE - how to plot the average sentiment\n",
        "# df = pd.read_csv(movie_results_path+'civil_war.csv')\n",
        "# avg_df = calculate_average_sentiment_per_consecutive_pair(df, annotated=False)\n",
        "# G = create_graph_average_sentiment_per_consecutive_pair(avg_df)\n",
        "# plot_average_sentiment_per_consecutive_pair(G)\n",
        "\n",
        "# # EXAMPLE - how to run my_min_cut_decomposition\n",
        "# df = pd.read_csv(movie_results_path+'civil_war.csv')\n",
        "# avg_df = calculate_average_sentiment_per_consecutive_pair(df, annotated=False)\n",
        "# G = create_graph_average_sentiment_per_consecutive_pair(avg_df)\n",
        "# my_min_cut_decomposition(G)"
      ],
      "metadata": {
        "id": "BQEEZzYkuzGN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize data. online and filterable!"
      ],
      "metadata": {
        "id": "FAOfzhDae3Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a plotly figure that has every line spoken/character/emotion plotted over the course of the movie\n",
        "## Example: https://chart-studio.plotly.com/~coffeeboost/1\n",
        "\n",
        "# fig = px.line(df, x=df.index, y='Sentiment', color='Name', markers=True, line_dash='Name', title='Line Plot for Each Unique Name',hover_data={'Name': True, 'Sentiment': True, 'Line': True})\n",
        "# fig.update_layout(xaxis_title='Index', yaxis_title='Value', legend_title='Name')\n",
        "# py.sign_in(username='USERNAME', api_key='API_KEY')\n",
        "# chart_studio_plot = py.plot(fig, filename='interactive_plot', auto_open=False)"
      ],
      "metadata": {
        "id": "_SUSpmTmf0Cn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create node colour list\n",
        "def create_community_node_colors(graph, communities):\n",
        "    colors = [\n",
        "    \"#FFD1DC\", \"#ADD8E6\", \"#98FB98\", \"#D8BFD8\", \"#FFDAB9\",\n",
        "    \"#F0E68C\", \"#FFB6C1\", \"#87CEFA\", \"#DDA0DD\", \"#00FF7F\",\n",
        "    \"#FFC0CB\", \"#AFEEEE\", \"#FFE4B5\", \"#E0FFFF\", \"#FAFAD2\",\n",
        "    \"#F5FFFA\", \"#FFA07A\", \"#AFEEEE\", \"#FFE4E1\", \"#F0FFF0\",\n",
        "    \"#FFF5EE\", \"#E6E6FA\", \"#FFFAF0\", \"#F0F8FF\", \"#FFF8DC\",\n",
        "    \"#FFEBCD\", \"#FFEFD5\", \"#F5F5F5\", \"#F0F0F0\", \"#FFFFFF\"]\n",
        "\n",
        "    node_colors = []\n",
        "    for node in graph:\n",
        "        current_community_index = 0\n",
        "        for community in communities:\n",
        "            if node in community:\n",
        "                node_colors.append(colors[current_community_index])\n",
        "                break\n",
        "            current_community_index += 1\n",
        "    return node_colors\n",
        "\n",
        "\n",
        "from networkx.drawing.nx_agraph import graphviz_layout\n",
        "# function to plot graph with node colouring based on communities\n",
        "def visualize_communities(graph, communities, i):\n",
        "    node_colors = create_community_node_colors(graph, communities)\n",
        "    title = f\"Community Visualization of {len(communities)} communities\"\n",
        "    pos = nx.spring_layout(graph, k=0.3, iterations=50, seed=2)\n",
        "\n",
        "    # Use force_atlas2_layout for better positioning without overlaps\n",
        "    pos = graphviz_layout(graph, prog=\"neato\", root=None)\n",
        "\n",
        "    plt.subplot(2, 1, i)\n",
        "    plt.title(title)\n",
        "    nx.draw(\n",
        "        graph,\n",
        "        pos=pos,\n",
        "        node_size=1000,\n",
        "        node_color=node_colors,\n",
        "        with_labels=True,\n",
        "        font_size=10,\n",
        "        font_color=\"black\",\n",
        "    )\n",
        "\n",
        "def find_communities(G, num_communities=3):\n",
        "  communities = list(nx.community.girvan_newman(G))\n",
        "  fig, ax = plt.subplots(1, figsize=(10, 15))\n",
        "  visualize_communities(G, communities[num_communities-2], 1)\n",
        "\n",
        "## EXAMPLE - Find communities using Girvan-Newman\n",
        "# find_communities(G)"
      ],
      "metadata": {
        "id": "DjtPgyoTSQ8h"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Processing\n",
        "- convert string to float\n",
        "- scale number betwee [-1,1]\n",
        "- remove consecutive characters\n",
        "- scale down  with low pair count\n",
        "- zero sclaing based on the median"
      ],
      "metadata": {
        "id": "mC1lRapos99X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert string to float\n",
        "def parse(string):\n",
        "  string = string[1:-1] # remove brackets\n",
        "  string = string.strip() # remove white spaces\n",
        "  split = string.split(' ')\n",
        "  num1 = split[0] # first\n",
        "  num2 = split[-1] # last\n",
        "  return float(num1), float(num2)\n",
        "\n",
        "# format string to float\n",
        "def format_string_to_float(df):\n",
        "  df['Irony'] = df['Irony'].apply(lambda x: parse(x)[0])\n",
        "  df['Hate'] = df['Hate'].apply(lambda x: parse(x)[1])\n",
        "  return df\n",
        "\n",
        "# do min max scaling [-1,1]\n",
        "def scale_to_minus_1_and_1(df):\n",
        "  min = df[EMOTION_COLUMNS].min().min()\n",
        "  max = df[EMOTION_COLUMNS].max().max()\n",
        "  for col_name in EMOTION_COLUMNS:\n",
        "    df[col_name] = df[col_name].apply(lambda x: 2 * (x - min)/( max - min) - 1)\n",
        "  return df\n",
        "\n",
        "# scale down pairs with low pair\n",
        "# Calculate the average sentiment for each pair of consecutive characters\n",
        "def get_pair_count(df, annotated):\n",
        "  pair_count = {}\n",
        "  length = len(df) if annotated else len(df)-1\n",
        "\n",
        "  for i in range(length):\n",
        "      current_char = df.iloc[i, df.columns.get_loc('Name')]\n",
        "      next_char = df.iloc[i, df.columns.get_loc('Spoken')] if annotated else df.iloc[i + 1, df.columns.get_loc('Name')]\n",
        "      pair = (current_char, next_char)\n",
        "      if pair in pair_count:\n",
        "        pair_count[pair] += 1\n",
        "      else:\n",
        "        pair_count[pair] = 1\n",
        "  return pair_count\n",
        "\n",
        "def scale_down_by_count(df, annotated):\n",
        "  pair_count = get_pair_count(df, annotated)\n",
        "  length = len(df) if annotated else len(df)-1\n",
        "\n",
        "  for i in range(length):\n",
        "    current_char = df.iloc[i, df.columns.get_loc('Name')]\n",
        "    next_char = df.iloc[i, df.columns.get_loc('Spoken')] if annotated else df.iloc[i + 1, df.columns.get_loc('Name')]\n",
        "\n",
        "    pair = (current_char, next_char)\n",
        "    count = pair_count[pair]\n",
        "\n",
        "    for column in EMOTION_COLUMNS:\n",
        "      df.iloc[i, df.columns.get_loc(column)] *= 1-1/(count+1)\n",
        "\n",
        "  return df\n",
        "\n",
        "def scale_down_by_log(df):\n",
        "  for emotion in EMOTION_COLUMNS:\n",
        "    df[emotion] = np.log(df[emotion])\n",
        "  return df\n",
        "\n",
        "def zero_scale_to_median(df):\n",
        "  for emotion in EMOTION_COLUMNS:\n",
        "    median = df[emotion].median()\n",
        "    df[emotion] -= median\n",
        "  return df"
      ],
      "metadata": {
        "id": "SGtmF4EuxJDC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results of analysis\n"
      ],
      "metadata": {
        "id": "d8V8tpgEsWMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(movie_results_path+'Wizard-of-Oz.csv')\n",
        "# prepare data for analysis\n",
        "df = format_string_to_float(df)\n",
        "df = scale_to_minus_1_and_1(df)\n",
        "df = scale_down_by_count(df, annotated=False)\n",
        "df = zero_scale_to_median(df)\n",
        "# prepare graph for analysis\n",
        "avg_df = calculate_average_sentiment_per_consecutive_pair(df, annotated=False)\n",
        "G = create_graph_average_sentiment_per_consecutive_pair(avg_df)"
      ],
      "metadata": {
        "id": "sNsjmAHlsXdt"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # run barrage of algorithms\n",
        "# plot_average_sentiment_per_consecutive_pair(G)\n",
        "# avg_df = calculate_average_sentiment_per_consecutive_pair(df, annotated=False, emotion='nervousness')\n",
        "# G = create_graph_average_sentiment_per_consecutive_pair(avg_df)\n",
        "# my_min_cut_decomposition(G, num_iterations=20)\n",
        "# find_communities(G, num_communities=16)"
      ],
      "metadata": {
        "id": "-bDqzB22V029"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xuzviIkIss-U",
        "RLX60qaIZgL2",
        "fYficGgMa66D",
        "ntCGiAwNk82e",
        "6mwGGSbkogkG",
        "gjPm-J09ciCl",
        "FAOfzhDae3Be"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}